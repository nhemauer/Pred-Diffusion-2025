{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8a0139",
   "metadata": {},
   "source": [
    "RF Script for Hemauer, Saunders, and Desmarais\n",
    "\n",
    "Last updated: 05/24/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef979025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "1440 fits failed out of a total of 4320.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "866 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "574 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ndhem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.96786992 0.96811554 0.96779221 0.96768982 0.966991   0.96753412\n",
      " 0.96723337 0.96734691 0.96720824 0.96744259 0.96737222 0.96727555\n",
      " 0.96715756 0.96740799 0.96726471 0.96738967 0.96710895 0.96734178\n",
      " 0.96746973 0.967383   0.9676263  0.96761631 0.96737379 0.96721315\n",
      " 0.96685691 0.96729822 0.96725404 0.96710495 0.96685691 0.96729822\n",
      " 0.96725404 0.96710495 0.9665914  0.96703444 0.96687496 0.9667488\n",
      " 0.96786992 0.96811554 0.96779221 0.96768982 0.966991   0.96753412\n",
      " 0.96723337 0.96734691 0.96720824 0.96744259 0.96737222 0.96727555\n",
      " 0.96715756 0.96740799 0.96726471 0.96738967 0.96710895 0.96734178\n",
      " 0.96746973 0.967383   0.9676263  0.96761631 0.96737379 0.96721315\n",
      " 0.96685691 0.96729822 0.96725404 0.96710495 0.96685691 0.96729822\n",
      " 0.96725404 0.96710495 0.9665914  0.96703444 0.96687496 0.9667488\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99251635 0.9926691  0.99269842 0.9927261  0.9923095  0.99238479\n",
      " 0.99243364 0.99246796 0.99147862 0.99161278 0.99170605 0.99173813\n",
      " 0.99213526 0.99223378 0.99230568 0.99232884 0.99197866 0.99211512\n",
      " 0.99215808 0.99221596 0.99130426 0.9914567  0.99149727 0.9915418\n",
      " 0.99093471 0.99111903 0.99119653 0.9912146  0.99093471 0.99111903\n",
      " 0.99119653 0.9912146  0.99076998 0.99094001 0.99098156 0.99101942\n",
      " 0.99251635 0.9926691  0.99269842 0.9927261  0.9923095  0.99238479\n",
      " 0.99243364 0.99246796 0.99147862 0.99161278 0.99170605 0.99173813\n",
      " 0.99213526 0.99223378 0.99230568 0.99232884 0.99197866 0.99211512\n",
      " 0.99215808 0.99221596 0.99130426 0.9914567  0.99149727 0.9915418\n",
      " 0.99093471 0.99111903 0.99119653 0.9912146  0.99093471 0.99111903\n",
      " 0.99119653 0.9912146  0.99076998 0.99094001 0.99098156 0.99101942\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99247806 0.99293612 0.99301594 0.99309877 0.9928183  0.99288517\n",
      " 0.99296546 0.99297498 0.99226288 0.99244596 0.99248731 0.99247975\n",
      " 0.99275558 0.99286309 0.99297446 0.99304079 0.99271584 0.99280724\n",
      " 0.9928836  0.99293585 0.99202179 0.99215817 0.99221162 0.99226733\n",
      " 0.99170678 0.99179837 0.99186984 0.99193218 0.99170678 0.99179837\n",
      " 0.99186984 0.99193218 0.99142495 0.99157011 0.9916375  0.99169148\n",
      " 0.99247806 0.99293612 0.99301594 0.99309877 0.9928183  0.99288517\n",
      " 0.99296546 0.99297498 0.99226288 0.99244596 0.99248731 0.99247975\n",
      " 0.99275558 0.99286309 0.99297446 0.99304079 0.99271584 0.99280724\n",
      " 0.9928836  0.99293585 0.99202179 0.99215817 0.99221162 0.99226733\n",
      " 0.99170678 0.99179837 0.99186984 0.99193218 0.99170678 0.99179837\n",
      " 0.99186984 0.99193218 0.99142495 0.99157011 0.9916375  0.99169148\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99250707 0.99287313 0.99295266 0.99302316 0.99280734 0.99291245\n",
      " 0.99294148 0.99294564 0.99225633 0.99238994 0.99243784 0.99246766\n",
      " 0.99278099 0.9929044  0.99301415 0.99307828 0.99272745 0.99285387\n",
      " 0.99289783 0.99294745 0.99200949 0.99215354 0.99220298 0.99227354\n",
      " 0.99168437 0.99180749 0.99187779 0.99193896 0.99168437 0.99180749\n",
      " 0.99187779 0.99193896 0.99138549 0.99154206 0.99161879 0.99168587\n",
      " 0.99250707 0.99287313 0.99295266 0.99302316 0.99280734 0.99291245\n",
      " 0.99294148 0.99294564 0.99225633 0.99238994 0.99243784 0.99246766\n",
      " 0.99278099 0.9929044  0.99301415 0.99307828 0.99272745 0.99285387\n",
      " 0.99289783 0.99294745 0.99200949 0.99215354 0.99220298 0.99227354\n",
      " 0.99168437 0.99180749 0.99187779 0.99193896 0.99168437 0.99180749\n",
      " 0.99187779 0.99193896 0.99138549 0.99154206 0.99161879 0.99168587\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.96811672 0.96859227 0.96840206 0.96843729 0.96823858 0.96858971\n",
      " 0.96842557 0.9683869  0.96798835 0.96854593 0.96819275 0.96815447\n",
      " 0.96802558 0.96849434 0.96828342 0.96831221 0.96789904 0.96837969\n",
      " 0.96835536 0.96839659 0.96795783 0.96838839 0.96813774 0.96818031\n",
      " 0.9676506  0.96798858 0.96792884 0.96801451 0.9676506  0.96798858\n",
      " 0.96792884 0.96801451 0.96793449 0.96803514 0.96797671 0.96798274\n",
      " 0.96811672 0.96859227 0.96840206 0.96843729 0.96823858 0.96858971\n",
      " 0.96842557 0.9683869  0.96798835 0.96854593 0.96819275 0.96815447\n",
      " 0.96802558 0.96849434 0.96828342 0.96831221 0.96789904 0.96837969\n",
      " 0.96835536 0.96839659 0.96795783 0.96838839 0.96813774 0.96818031\n",
      " 0.9676506  0.96798858 0.96792884 0.96801451 0.9676506  0.96798858\n",
      " 0.96792884 0.96801451 0.96793449 0.96803514 0.96797671 0.96798274\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99250541 0.99265232 0.99270874 0.99276557 0.99283153 0.99292312\n",
      " 0.99295637 0.99301129 0.99257958 0.99271226 0.99271704 0.9927469\n",
      " 0.99291398 0.99301941 0.9930335  0.99303975 0.99275608 0.99285416\n",
      " 0.99289555 0.99295061 0.99243701 0.99252329 0.99260086 0.99262025\n",
      " 0.99231712 0.99233892 0.99242246 0.99246524 0.99231712 0.99233892\n",
      " 0.99242246 0.99246524 0.99222295 0.99231549 0.99232309 0.9923531\n",
      " 0.99250541 0.99265232 0.99270874 0.99276557 0.99283153 0.99292312\n",
      " 0.99295637 0.99301129 0.99257958 0.99271226 0.99271704 0.9927469\n",
      " 0.99291398 0.99301941 0.9930335  0.99303975 0.99275608 0.99285416\n",
      " 0.99289555 0.99295061 0.99243701 0.99252329 0.99260086 0.99262025\n",
      " 0.99231712 0.99233892 0.99242246 0.99246524 0.99231712 0.99233892\n",
      " 0.99242246 0.99246524 0.99222295 0.99231549 0.99232309 0.9923531\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99133885 0.99198116 0.99216023 0.9922761  0.99323464 0.99327131\n",
      " 0.99333587 0.99338423 0.99301395 0.99313066 0.99322071 0.99327424\n",
      " 0.99352002 0.99359021 0.99365918 0.99369336 0.99339216 0.9935047\n",
      " 0.99357467 0.99362138 0.99316049 0.99323221 0.99327406 0.99330555\n",
      " 0.99301642 0.99306756 0.99313048 0.99318417 0.99301642 0.99306756\n",
      " 0.99313048 0.99318417 0.99291639 0.99299131 0.99304123 0.99304764\n",
      " 0.99133885 0.99198116 0.99216023 0.9922761  0.99323464 0.99327131\n",
      " 0.99333587 0.99338423 0.99301395 0.99313066 0.99322071 0.99327424\n",
      " 0.99352002 0.99359021 0.99365918 0.99369336 0.99339216 0.9935047\n",
      " 0.99357467 0.99362138 0.99316049 0.99323221 0.99327406 0.99330555\n",
      " 0.99301642 0.99306756 0.99313048 0.99318417 0.99301642 0.99306756\n",
      " 0.99313048 0.99318417 0.99291639 0.99299131 0.99304123 0.99304764\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99100542 0.99158749 0.99172474 0.99187545 0.99317561 0.99323832\n",
      " 0.9933164  0.99330577 0.99298153 0.99309609 0.99316152 0.99318907\n",
      " 0.99350279 0.99362407 0.99370222 0.99373522 0.99341527 0.99351376\n",
      " 0.99358882 0.99365027 0.99313536 0.99321546 0.99326407 0.9933049\n",
      " 0.99306259 0.993084   0.99314917 0.99320246 0.99306259 0.993084\n",
      " 0.99314917 0.99320246 0.99293634 0.99299452 0.99302701 0.99304518\n",
      " 0.99100542 0.99158749 0.99172474 0.99187545 0.99317561 0.99323832\n",
      " 0.9933164  0.99330577 0.99298153 0.99309609 0.99316152 0.99318907\n",
      " 0.99350279 0.99362407 0.99370222 0.99373522 0.99341527 0.99351376\n",
      " 0.99358882 0.99365027 0.99313536 0.99321546 0.99326407 0.9933049\n",
      " 0.99306259 0.993084   0.99314917 0.99320246 0.99306259 0.993084\n",
      " 0.99314917 0.99320246 0.99293634 0.99299452 0.99302701 0.99304518]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 4010.10 seconds\n",
      "F1 Macro: 0.5762065050049979\n",
      "Balanced Accuracy: 0.5574436297092547\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97      8448\n",
      "         1.0       0.30      0.13      0.18       444\n",
      "\n",
      "    accuracy                           0.94      8892\n",
      "   macro avg       0.63      0.56      0.58      8892\n",
      "weighted avg       0.92      0.94      0.93      8892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "# Data\n",
    "boehmke_2017_full = pd.read_stata(r\"boehmke_analysis\\replication_data\\boehmke2017.dta\")\n",
    "\n",
    "covariates = [\"srcs_decay\",\"nbrs_lag\",\"rpcpinc\",\"totpop\",\"legp_squire\",\n",
    "                \"citi6010\",\"unif_rep\",\"unif_dem\",\"time\",\"time_sq\",\"time_cube\"]\n",
    "boehmke_2017 = boehmke_2017_full[[\"state\", \"year\", \"statepol\", \"adopt\"] + covariates].dropna()\n",
    "\n",
    "# Factor DV\n",
    "boehmke_2017['state'] = boehmke_2017['state'].astype('category')\n",
    "\n",
    "# Encode 'state' as numeric codes for modeling\n",
    "X = boehmke_2017.drop('adopt', axis = 1).copy()\n",
    "X['state'] = X['state'].cat.codes\n",
    "y = boehmke_2017['adopt']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1337, stratify = y)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state = 1337)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit a Random Forest Classifier with hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [10, 20, 30, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_boehmke_2017 = RandomForestClassifier(random_state = 1337)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = rf_boehmke_2017,\n",
    "    param_grid = param_grid,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    verbose = 2,\n",
    "    scoring = 'average_precision'  # This should be correct for imbalanced datasets (?)\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"GridSearchCV took {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Predict\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Test Statistics\n",
    "f1_macro = f1_score(y_test, y_pred, average = 'macro')\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Macro: {f1_macro}\")\n",
    "print(f\"Balanced Accuracy: {balanced_acc}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
